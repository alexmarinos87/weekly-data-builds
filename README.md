# Roadâ€‘Map: Weekly GitHub Projects for Aspiring Data Engineers

*VersionÂ 1.0 â€“ JulyÂ 2025*

---

## How to Use This Plan

1. **Create a private or public GitHub *organisation* named `weekly-data-builds`** (or similar).
2. **Fork the included *Template Repository*** (`deâ€‘template`) containing:

   * `.github/workflows/ci.yml` â€“ runs `black`, `ruff`, `pytest`, and uploads coverage.
   * `tasks.py` â€“ invoke tasks: `clean`, `test`, `lint`, `run`.
   * `README_TEMPLATE.md`, `LICENSE`, `.gitignore`, `CODE_OF_CONDUCT.md`.
3. **Each Monday** click *Use this template* â†’ name the repo `YYYYâ€‘WW_<slug>` (e.g. `2025â€‘29_batchâ€‘etl`).
4. Follow the sprint checklist (p.Â 4). Merge to *main* by Sunday night and tag `v0.1.0`.
5. After every four projects, record a 3â€‘min demo video and pin it to LinkedIn / your portfolio site.

---

## Weekly Sprint Checklist (applies to every project)

| When                  | Deliverable            | Details                                                         |
| --------------------- | ---------------------- | --------------------------------------------------------------- |
| **Mon â€¨(1Â h)**        | *Idea & ProblemÂ Brief* | Fill out `docs/oneâ€‘pager.md`: problem, scope, success criteria. |
| **Tue (15Â m)**        | *Repo Scaffold*        | Initialise from template, push first commit.                    |
| **TueÂ â€“Â Wed (2â€‘3Â h)** | *Core Build*           | Write minimal working pipeline + unit tests.                    |
| **Thu (1Â h)**         | *Polish*               | Add logging, type hints, proper error handling.                 |
| **Fri (1Â h)**         | *Docs & Demo*          | Update README, record asciinema / GIF, push schema diagram.     |
| **Sat (20Â m)**        | *Review & Tag*         | Selfâ€‘review PR â†’ squash & merge â†’ `git tag -a v0.1.0`.          |
| **Sun (10Â m)**        | *Retro & Share*        | Log takeaway in `CHANGELOG.md`, share link on social.           |

---

## 26â€‘Week Project Calendar

| Week | Theme & Repository                        | Core Tech                  | Learning Outcome                                    |
| ---- | ----------------------------------------- | -------------------------- | --------------------------------------------------- |
| 1    | **Batchâ€‘ETLÂ Starter** `YYYYâ€‘WW_batchâ€‘etl` | Python, Pandas, DuckDB     | CLI loads CSV â†’ Parquet â†’ DuckDB; introduce `COPY`. |
| 2    | **dbtÂ Basics**                            | dbtÂ 1.8, Postgres          | Starâ€‘schema modelling, tests, docs site.            |
| 3    | **AirflowÂ 101**                           | AirflowÂ 2.9                | DAG to orchestrate WeeksÂ 1â€‘2 pipeline.              |
| 4    | **DataÂ Quality**                          | Great Expectations         | Create expectations, integrate with Airflow.        |
| 5    | **IncrementalÂ Loads**                     | Postgres partitions        | Upsert pattern (`INSERT â€¦ ON CONFLICT`).            |
| 6    | **Dashboard**                             | Metabase / Superset        | Connect to warehouse, build KPI board.              |
| 7    | **Changeâ€‘Dataâ€‘Capture**                   | Debezium, Kafka            | Stream Postgres updates to topic.                   |
| 8    | **StreamÂ Processing**                     | Kafka Streams / FlinkÂ SQL  | Aggregate CDC stream to rolling metrics.            |
| 9    | **Lowâ€‘Latency OLAP**                      | ClickHouse                 | Consume topic, query latency <Â 5Â s.                 |
| 10   | **Lineage & Metadata**                    | OpenLineage + Marquez      | Track job lineage and surface in UI.                |
| 11   | **Lakehouse TableÂ Format**                | Apache Iceberg             | Write partition evolution demo.                     |
| 12   | **Orchestrated Lakehouse**                | SparkÂ StructuredÂ Streaming | Stream into Iceberg, query via Trino.               |
| 13   | **VectorÂ DB Basics**                      | pgvector                   | Store & query embeddings; similarity search API.    |
| 14   | **RAGÂ Service**                           | FastAPI + LangChain        | Chat endpoint using WeekÂ 13 embeddings.             |
| 15   | **FeatureÂ StoreÂ Intro**                   | Feast + Redis              | Offline & online feature parity tests.              |
| 16   | **MLÂ Pipeline**                           | scikitâ€‘learn + mlflow      | Train model on Feast features; register & serve.    |
| 17   | **Observability**                         | Prometheus, Grafana        | Export query latency & DAG metrics, set alerts.     |
| 18   | **CostÂ Benchmark**                        | Snowflake vs BigQuery      | Load 10Â GB and compare cost/perf.                   |
| 19   | **Security & RBAC**                       | Postgres RLS, Vault        | Implement rowâ€‘level security and secret rotation.   |
| 20   | **DataÂ Contracts**                        | protobuf / pandera         | Validate schema changes in CI.                      |
| 21   | **Serverless Batch**                      | AWSÂ Lambda + S3            | Microâ€‘ETL under 60Â s cold start.                    |
| 22   | **IaCÂ Foundations**                       | Terraform                  | Oneâ€‘command deploy of WeekÂ 21.                      |
| 23   | **PulumiÂ Python**                         | Pulumi                     | Reâ€‘implement IaC in Python for comparison.          |
| 24   | **DistributedÂ Testing**                   | pytestâ€‘xdist               | Parallel tests for large datasets.                  |
| 25   | **ScalingÂ Challenge**                     | k6 / Locust                | Loadâ€‘test pipeline; produce report.                 |
| 26   | **Capstone Refactor**                     | Pick 3 favourite projects  | Integrate, add endâ€‘toâ€‘end docs, present demo.       |

---

## README Scaffold (quick copyâ€‘paste)

````markdown
# <ProjectÂ Title>

> Oneâ€‘sentence summary of what this solves.

## Architecture
![diagram](docs/architecture.png)

## QuickÂ Start
```bash
make up    # spin up Docker services
make load  # run the ETL job
make test  # execute tests + coverage
````

## Configuration

| Variable | Description                | Default        |
| -------- | -------------------------- | -------------- |
| `DB_URL` | Postgres connection string | `postgres://â€¦` |

## Development

Explain branching strategy, preâ€‘commit hooks, how to run linters.

## License

MIT Â©Â YourÂ Name 2025

````

---
## Evaluation Metrics (include in every README)
| Metric | Target |
|--------|--------|
| **DataÂ freshness** | <Â 5Â min lag from source to warehouse |
| **Pipeline success rate** | â‰¥Â 99Â % last 30Â days |
| **Test coverage** | â‰¥Â 85Â % lines |
| **Cost per 1Â GB processed** | Benchmark and record |

---
## Recommended Tool Versions (as of JulyÂ 2025)
- PythonÂ 3.12
- PostgresÂ 15.5
- dbtÂ 1.8.1
- ApacheÂ AirflowÂ 2.9
- KafkaÂ 3.7
- SparkÂ 3.6
- IcebergÂ 1.5

Update as new stable releases appear.

---
## Further Reading & Resources
1. *Designing Dataâ€‘Intensive Applications* â€“Â MartinÂ Kleppmann.
2. dbt *Analytics Engineering* Guide.
3. Confluent *Streaming Platform* Training.
4. Lakehouse WhiteÂ Paper â€“ Applecart 2025.
5. Great ExpectationsÂ v0.18 Docs.

---
### Author
**<Your Name>** â€“ dataâ€‘engineering enthusiast.

*Feel free to fork, star, and raise PRsâ€”improvements welcome!*

---
## Example `README.md` for a Weekly Project Repo
Below is a readyâ€‘toâ€‘copy README you can drop into any individual project repository (e.g. `2025â€‘29_batchâ€‘etl`). It expands the scaffold with concrete sections and badges.

```markdown
# Batchâ€‘ETL Starter ðŸš€

![CI](https://github.com/<yourâ€‘org>/2025â€‘29_batchâ€‘etl/actions/workflows/ci.yml/badge.svg)
![License](https://img.shields.io/badge/license-MIT-green)

A minimal, productionâ€‘style batch pipeline that ingests raw CSV files, converts them to columnar *Parquet*, and loads them into **DuckDB** for instant analytics.

---
## Architecture
```mermaid
graph TD;
    A[Raw CSV on disk] -->|invoke tasks.load| B[Parquet Staging];
    B --> C[DuckDB Warehouse];
    C --> D[Metabase Dashboard];
````

---

## QuickÂ Start

```bash
# 1. Create virtual env & install deps
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt

# 2. Run the pipeline
make load INPUT=data/raw OUTPUT=data/warehouse

# 3. Query the data
python - <<'PY'
import duckdb, pandas as pd
db = duckdb.connect('data/warehouse/etl.duckdb')
print(db.execute('SELECT COUNT(*) FROM trips').fetchall())
PY
```

---

## Configuration

| Variable    | Description                     | Default          |
| ----------- | ------------------------------- | ---------------- |
| `INPUT`     | Folder containing raw CSV files | `data/raw`       |
| `OUTPUT`    | DuckDB db path / Parquet folder | `data/warehouse` |
| `LOG_LEVEL` | Python logging level            | `INFO`           |

All variables can be set via env vars or passed as `make` arguments.

---

## Project Structure

```
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/          # sample CSVs
â”‚   â””â”€â”€ warehouse/    # DuckDB & Parquet output
â”œâ”€â”€ etl/
â”‚   â”œâ”€â”€ loader.py     # file â†’ parquet
â”‚   â””â”€â”€ models.py     # DuckDB DDL
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_etl.py
â”œâ”€â”€ tasks.py          # invoke tasks
â””â”€â”€ docker-compose.yml
```

---

## Development

1. **Coding standard** â€“ `black`, `ruff`, `mypy` run in CI and preâ€‘commit hooks.
2. **Testing** â€“ `pytest -q`; coverage â‰¥ 90Â %.
3. **Branching** â€“ feature branches â†’ PR â†’ squash & merge â†’ tag.

---

## Roadmap

* [ ] Add incremental load mode
* [ ] Integrate Great Expectations tests
* [ ] Switch to S3 staging bucket

Have suggestions? Open an issue or PR!

---

## License

[MIT](LICENSE) Â©Â <Your Name> 2025

```

Use this as the default README for each new weekly repoâ€”replace the title, diagram, and quickâ€‘start commands as needed.

```
