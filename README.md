Batchâ€‘ETL Starter ðŸš€

Lightweight CLI that ingests raw CSV files, converts them to columnar Parquet, and loads the result into DuckDB for instant analytics.

QuickÂ Start

# 1. Install dependencies
poetry install --all-extras  # or: pip install -r requirements.txt

# 2. Run the pipeline
python -m etl.cli run data/raw/sample.csv

# 3. Query the warehouse
duckdb data/duckdb/batch.db \
  "SELECT * FROM batch_stage LIMIT 10;"

ProjectÂ Structure

.
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/          # source CSVs
â”‚   â””â”€â”€ staging/      # generated Parquet files
â”œâ”€â”€ etl/              # importable Python package
â”‚   â”œâ”€â”€ io.py         # csvâ†”parquetâ†”duckdb helpers
â”‚   â””â”€â”€ cli.py        # Typer CLI entrypoint
â”œâ”€â”€ tests/            # pytest unit tests
â””â”€â”€ pyproject.toml    # pinned runtime + dev deps

DevelopmentÂ Workflow

Task

Command

Format &Â Lint

pre-commit run --all-files

Unit Tests

pytest -q

CLI Help

python -m etl.cli --help

Continuous Integration (GitHub Actions) runs lint and tests on every push.

Requirements

Tool

Version

Python

3.12

Pandas

2.2

DuckDB

0.10

License

MIT Â©Â 2025Â 

